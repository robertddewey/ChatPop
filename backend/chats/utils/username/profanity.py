#!/usr/bin/env python3
"""
username_validator.py

Username validation system that:
1. Enforces username format rules (length, allowed characters)
2. Normalizes leet speak to detect obfuscated banned words
3. Uses allowlists to reduce false positives
4. Checks against banned words list

Username Rules:
- Minimum length: 5 characters
- Maximum length: 15 characters
- Allowed characters: letters (a-z, A-Z), numbers (0-9), and underscores (_)
- No spaces allowed
- Case is preserved but doesn't count toward uniqueness

Usage:
    from username_validator import is_username_allowed

    result = is_username_allowed("P4ssw0rd_123")
    if result.allowed:
        print("Username is allowed")
    else:
        print(f"Username rejected: {result.reason}")
"""

from __future__ import annotations

import json
import os
import re
from dataclasses import dataclass
from typing import Optional, Set

# Import your banned words
try:
    from .words import BANNED_WORDS
except ImportError:
    # Fallback for testing
    BANNED_WORDS = [
        "ass",
        "cum",
        "tit",
        "sex",
        "dick",
        "cock",
        "pussy",
        "fuck",
        "fucking",
        "fucker",
        "nigger",
        "nigga",
        "faggot",
        "slut",
        "whore",
        "shemale",
        "rape",
        "rapist",
        "cocaine",
        "porno",
        "pornography",
        "porn",
        "xxx",
        "hentai",
        "anal",
        "cunt",
        "shit",
        "bitch",
        "damn",
        "hell",
        "crap",
        "piss",
        "bastard",
        "asshole",
    ]


def load_allowlists(json_path: str = "allowlists.json") -> tuple[Set[str], Set[str]]:
    """Load allowlists from JSON file generated by build_allowlists.py"""
    try:
        # Try current directory first
        if os.path.exists(json_path):
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        else:
            # Try relative to this script's directory
            script_dir = os.path.dirname(os.path.abspath(__file__))
            full_path = os.path.join(script_dir, json_path)
            if os.path.exists(full_path):
                with open(full_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
            else:
                print(f"[warning] Allowlists file not found at {json_path} or {full_path}")
                return set(), set()

        allowlist_tokens = set(data.get("ALLOWLIST_TOKENS", []))
        allowlist_substrings = set(data.get("ALLOWLIST_SUBSTRINGS", []))

        print(f"[info] Loaded allowlists: {len(allowlist_tokens)} tokens, {len(allowlist_substrings)} substrings")
        return allowlist_tokens, allowlist_substrings

    except Exception as e:
        print(f"[error] Failed to load allowlists from {json_path}: {e}")
        print("[info] Using fallback allowlists")
        return get_fallback_allowlists()


def get_fallback_allowlists() -> tuple[Set[str], Set[str]]:
    """Fallback allowlists if JSON file can't be loaded"""
    fallback_tokens = {
        "pass",
        "password",
        "passenger",
        "compass",
        "class",
        "classroom",
        "glasses",
        "grass",
        "mass",
        "amass",
        "broadcast",
        "asset",
        "assets",
        "assess",
        "assessment",
        "assistant",
        "assist",
        "associate",
        "association",
        "assert",
        "assignment",
        "assembly",
        "assume",
        "assumption",
        "embarrass",
        "reassure",
        "cumlaude",
        "cumulative",
        "accumulate",
        "accumulation",
        "accumulator",
        "cumin",
        "title",
        "titles",
        "entitle",
        "entitled",
        "entitlement",
        "titan",
        "titanium",
        "utility",
        "utilities",
        "legit",
        "partition",
        "stitched",
        "sussex",
        "essex",
        "middlesex",
        "dyslexia",
        "asexual",
        "intersection",
        "pen",
        "pencil",
        "pencils",
        "peninsula",
        "penal",
        "penalty",
        "penmanship",
        "dictionary",
        "dictionaries",
        "diction",
        "dictation",
        "predict",
        "prediction",
        "indicative",
        "indicate",
        "indication",
        "verdict",
        "jurisdiction",
        "edict",
        "octopus",
        "octopi",
        "platypus",
        "corpus",
        "corpora",
        "campus",
        "campuses",
        "cocoa",
        "coconut",
        "coconuts",
        "cocoon",
        "coccyx",
        "cocci",
        "focus",
        "focused",
        "focusing",
        "focuses",
        "fuchsia",
        "confucian",
        "japan",
        "japanese",
        "japonica",
        "nippon",
        "snippet",
        "snippets",
    }

    fallback_substrings = {"ass", "cum", "tit", "sex", "pen", "dic", "pus", "coc"}

    return fallback_tokens, fallback_substrings


# Load allowlists on module import
ALLOWLIST_TOKENS, ALLOWLIST_SUBSTRINGS = load_allowlists()


@dataclass
class ValidationResult:
    """Result of username validation"""

    allowed: bool
    reason: Optional[str] = None
    normalized_username: Optional[str] = None
    detected_words: Optional[Set[str]] = None
    canonical_form: Optional[str] = None  # lowercase for uniqueness checking


class UsernameValidator:
    """Validates usernames against format rules and banned words with leet speak normalization"""

    # Username format constraints
    MIN_LENGTH = 5
    MAX_LENGTH = 15
    ALLOWED_CHARS_PATTERN = re.compile(r"^[a-zA-Z0-9_]+$")

    # Comprehensive leet speak mapping - ordered by priority for multi-char replacements
    LEET_MAP = {
        # Multi-character substitutions (process these first)
        "ph": "f",  # "phuck" -> "fuck"
        "ck": "k",  # "fuk" variants, "suk" -> "suck"
        "kk": "ck",  # "fukk" -> "fuck"
        "xx": "ck",  # "fuxx" -> "fuck"
        # Number to letter (multiple possibilities - we'll handle this specially)
        "0": "o",  # Most common: zero -> O
        "1": "i",  # Most common: one -> I (but also 'l')
        "2": "z",  # "2" -> "z" (to, too)
        "3": "e",  # "3" -> "e"
        "4": "a",  # "4" -> "a" (for)
        "5": "s",  # "5" -> "s"
        "6": "g",  # "6" -> "g" or "b"
        "7": "t",  # "7" -> "t"
        "8": "b",  # "8" -> "b"
        "9": "g",  # "9" -> "g" or "q"
        # Common word-level substitutions
        "z": "s",  # plural forms, "iz" -> "is"
        "x": "ck",  # "sux" -> "suck", "sex" stays
        "q": "g",  # visual similarity
    }

    def __init__(
        self, banned_words: Set[str] = None, allowlist_tokens: Set[str] = None, allowlist_substrings: Set[str] = None
    ):
        # Convert banned_words to set if it's a list
        if banned_words is None:
            banned_words = set(BANNED_WORDS) if isinstance(BANNED_WORDS, list) else BANNED_WORDS
        else:
            banned_words = set(banned_words) if isinstance(banned_words, list) else banned_words

        self.banned_words = banned_words
        self.allowlist_tokens = allowlist_tokens or ALLOWLIST_TOKENS
        self.allowlist_substrings = allowlist_substrings or ALLOWLIST_SUBSTRINGS

        # Precompute normalized banned words for efficiency
        self.normalized_banned = {self.normalize_text(word) for word in self.banned_words}
        self.normalized_banned.update(self.banned_words)  # Keep originals too

        # Precompute normalized allowlist tokens for performance
        self.normalized_allowlist = {self.normalize_text(token).lower(): token for token in self.allowlist_tokens}

    def validate_format(self, username: str) -> Optional[str]:
        """Validate username format rules. Returns error message if invalid, None if valid."""
        if not username:
            return "Username cannot be empty"

        if len(username) < self.MIN_LENGTH:
            return f"Username must be at least {self.MIN_LENGTH} characters long"

        if len(username) > self.MAX_LENGTH:
            return f"Username must be no more than {self.MAX_LENGTH} characters long"

        if not self.ALLOWED_CHARS_PATTERN.match(username):
            return "Username can only contain letters (a-z, A-Z), numbers (0-9), and underscores (_)"

        if " " in username:
            return "Username cannot contain spaces"

        return None

    def get_canonical_form(self, username: str) -> str:
        """Get canonical lowercase form for uniqueness checking"""
        return username.lower()

    def normalize_text(self, text: str) -> str:
        """Convert leet speak and obfuscated text to normal characters"""
        if not text:
            return ""

        text = text.lower()

        # Remove underscores for content analysis (but keep for format validation)
        text = text.replace("_", "")

        # Process multi-character leet substitutions first (longer patterns)
        multi_char_leet = {k: v for k, v in self.LEET_MAP.items() if len(k) > 1}
        for leet_char, normal_char in sorted(multi_char_leet.items(), key=lambda x: len(x[0]), reverse=True):
            text = text.replace(leet_char, normal_char)

        # Then process single character substitutions
        single_char_leet = {k: v for k, v in self.LEET_MAP.items() if len(k) == 1}
        for leet_char, normal_char in single_char_leet.items():
            text = text.replace(leet_char, normal_char)

        # Handle repeated characters (like "assss" -> "ass")
        text = re.sub(r"(.)\1{2,}", r"\1\1", text)

        return text

    def normalize_text_variants(self, text: str) -> Set[str]:
        """Generate multiple normalization variants for ambiguous characters"""
        if not text:
            return set()

        variants = set()
        text = text.lower()

        # Remove underscores for content analysis
        text = text.replace("_", "")

        # Handle ambiguous number mappings - generate all combinations
        # Numbers that can map to multiple letters
        ambiguous_mappings = {
            "0": ["o", "0"],  # keep original too
            "1": ["i", "l", "1"],  # 1 can be i, l, or stay as 1
            "3": ["e", "3"],
            "4": ["a", "4"],
            "5": ["s", "5"],
            "6": ["g", "b", "6"],
            "7": ["t", "7"],
            "8": ["b", "8"],
            "9": ["g", "q", "9"],
        }

        # Start with the base text
        current_variants = {text}

        # For each ambiguous character, multiply variants
        for char, replacements in ambiguous_mappings.items():
            if char in text:
                new_variants = set()
                for variant in current_variants:
                    for replacement in replacements:
                        new_variants.add(variant.replace(char, replacement))
                current_variants = new_variants

                # Limit explosion - if too many variants, keep most promising ones
                if len(current_variants) > 20:
                    # Prioritize variants with more letters vs numbers
                    scored_variants = []
                    for v in current_variants:
                        letter_count = sum(1 for c in v if c.isalpha())
                        scored_variants.append((letter_count, v))
                    scored_variants.sort(reverse=True)
                    current_variants = {v for _, v in scored_variants[:20]}

        # Apply single-char leet mappings to all variants
        final_variants = set()
        single_char_leet = {k: v for k, v in self.LEET_MAP.items() if len(k) == 1 and k not in ambiguous_mappings}

        for variant in current_variants:
            # Apply multi-char substitutions first
            multi_char_leet = {k: v for k, v in self.LEET_MAP.items() if len(k) > 1}
            for leet_char, normal_char in sorted(multi_char_leet.items(), key=lambda x: len(x[0]), reverse=True):
                variant = variant.replace(leet_char, normal_char)

            # Apply single char (non-ambiguous) substitutions
            for leet_char, normal_char in single_char_leet.items():
                variant = variant.replace(leet_char, normal_char)

            # Handle repeated characters
            variant = re.sub(r"(.)\1{2,}", r"\1\1", variant)

            final_variants.add(variant)

        return final_variants

    def extract_words(self, text: str) -> Set[str]:
        """
        Extract potential words from text using various strategies.
        Enhanced to be more conservative and avoid over-fragmenting.
        """
        if not text:
            return set()

        words = set()

        # Add the full text if it's reasonable length
        if 3 <= len(text) <= 15:
            words.add(text)

        # Extract continuous alphabetic sequences
        alpha_sequences = re.findall(r"[a-z]+", text.lower())
        for seq in alpha_sequences:
            if len(seq) >= 3:
                words.add(seq)

        # Extract sliding windows, but be more conservative
        # Only check windows that might be actual words (3-8 chars usually)
        max_window = min(len(text), 8)
        for length in range(3, max_window + 1):
            for i in range(len(text) - length + 1):
                substring = text[i : i + length]
                if re.match(r"^[a-z]+$", substring):
                    words.add(substring)

        return words

    def is_allowed_by_allowlist(self, text: str) -> bool:
        """
        Enhanced allowlist checking with better normalization handling.
        """
        if not text:
            return False

        text_lower = text.lower()

        # Direct token match
        if text_lower in self.allowlist_tokens:
            return True

        # Remove underscores and check again
        text_clean = text_lower.replace("_", "")
        if text_clean in self.allowlist_tokens:
            return True

        # Check if any allowlisted substring provides coverage
        for allowed_sub in self.allowlist_substrings:
            if allowed_sub in text_lower:
                # If the substring covers most of the word, consider it safe
                remaining = text_lower.replace(allowed_sub, "", 1)
                if len(remaining) <= 2:
                    return True

        return False

    def find_banned_words(self, text: str) -> Set[str]:
        """
        Find banned words in the normalized text.
        Uses allowlists to prevent false positives on legitimate words.
        Simplified to avoid performance issues.
        """
        found_banned = set()

        # Step 1: Normalize the input text (single normalization, no variants for performance)
        normalized_text = self.normalize_text(text).lower()

        # Step 2: Early exit if the entire normalized text is explicitly allowlisted
        if normalized_text in self.allowlist_tokens:
            return set()  # Entire username is allowlisted

        # Step 3: Check for direct matches against banned words
        if normalized_text in self.normalized_banned:
            found_banned.add(normalized_text)
            return found_banned

        # Step 4: Check for banned word substrings
        for banned_word in self.normalized_banned:
            if len(banned_word) >= 3 and banned_word in normalized_text:
                # Found a banned substring, but check allowlist protection
                # Check if any allowlisted word would explain this match
                is_protected = False
                for normalized_allowed in self.normalized_allowlist.keys():
                    # If the normalized allowed token contains the banned word AND
                    # the normalized allowed token is present in our text, it's protected
                    if banned_word in normalized_allowed and normalized_allowed in normalized_text:
                        is_protected = True
                        break

                if not is_protected:
                    found_banned.add(banned_word)

        return found_banned

    def validate_username(self, username: str) -> ValidationResult:
        """Main validation function"""
        # First check format rules
        format_error = self.validate_format(username)
        if format_error:
            return ValidationResult(False, format_error)

        # Get canonical form for uniqueness
        canonical = self.get_canonical_form(username)

        # Check for banned content
        banned_found = self.find_banned_words(username)

        if banned_found:
            return ValidationResult(
                allowed=False,
                reason=f"Contains prohibited content: {', '.join(sorted(banned_found))}",
                normalized_username=self.normalize_text(username),
                detected_words=banned_found,
                canonical_form=canonical,
            )

        return ValidationResult(
            allowed=True, normalized_username=self.normalize_text(username), canonical_form=canonical
        )


# Global validator instance
_validator = UsernameValidator()


def is_username_allowed(username: str) -> ValidationResult:
    """Convenience function to validate a username"""
    return _validator.validate_username(username)


def get_canonical_username(username: str) -> str:
    """Get canonical form for uniqueness checking"""
    return _validator.get_canonical_form(username)


def configure_validator(
    banned_words: Set[str] = None,
    allowlist_tokens: Set[str] = None,
    allowlist_substrings: Set[str] = None,
    allowlist_json_path: str = None,
):
    """Reconfigure the global validator with custom word lists"""
    global _validator

    # Load from JSON if path provided
    if allowlist_json_path:
        tokens, substrings = load_allowlists(allowlist_json_path)
        allowlist_tokens = allowlist_tokens or tokens
        allowlist_substrings = allowlist_substrings or substrings

    _validator = UsernameValidator(banned_words, allowlist_tokens, allowlist_substrings)


def reload_allowlists(json_path: str = "allowlists.json"):
    """Reload allowlists from JSON file and reconfigure validator"""
    global ALLOWLIST_TOKENS, ALLOWLIST_SUBSTRINGS, _validator
    ALLOWLIST_TOKENS, ALLOWLIST_SUBSTRINGS = load_allowlists(json_path)
    _validator = UsernameValidator()
    print(f"[info] Reloaded allowlists from {json_path}")


# Example usage and testing
if __name__ == "__main__":
    # Show what allowlists were loaded
    print(f"Loaded allowlists: {len(ALLOWLIST_TOKENS)} tokens, {len(ALLOWLIST_SUBSTRINGS)} substrings")
    print(f"Allowlisted substrings: {sorted(ALLOWLIST_SUBSTRINGS)}")
    print()

    test_cases = [
        # Format validation tests
        ("user", False, "too short"),
        ("a", False, "too short"),
        ("this_username_is_way_too_long_for_our_system", False, "too long"),
        ("user name", False, "contains space"),
        ("user@name", False, "invalid character"),
        ("user-name", False, "invalid character"),
        ("user.name", False, "invalid character"),
        # Valid format tests
        ("user1", True, "valid format"),
        ("User_123", True, "valid format"),
        ("MyUser_99", True, "valid format"),
        ("a1b2c", True, "valid format"),
        ("User_Name_1", True, "valid format"),
        # Should be allowed (legitimate words)
        ("password123", True, "legitimate word"),
        ("class1cal", True, "classical with 1->i"),
        ("assess_me", True, "legitimate word"),
        ("assist_bot", True, "legitimate word"),
        ("compass_1", True, "legitimate word"),
        ("titan_99", True, "legitimate word"),
        # Should be blocked (banned words with various leet speak)
        ("4ss_h0le", False, "ass hole with leet"),
        ("p0rn_star", False, "porn with leet"),
        ("c0ck_99", False, "cock with leet"),
        ("fuk_u", False, "fuck variant"),
        ("fucking_1", False, "fucking"),
        ("d1ck_head", False, "dick with leet"),
        ("5h1t_face", False, "shit with leet"),
        ("pu55y_cat", False, "pussy with leet"),
        # Advanced leet speak
        ("4ss013", False, "asshole with complex leet"),
        ("d1ckh3ad", False, "dickhead with leet"),
        ("fuk1ng_99", False, "fucking with leet"),
        # Number ambiguity tests
        ("ass1st_me", True, "assist with 1->i"),
        ("c1ass_a", True, "class with 1->l"),
        ("d1ct10n", True, "dictionary variant"),
        ("tit1e_99", True, "title with 1->l"),
        # Edge cases
        ("p4ssw0rd", True, "password with leet"),
        ("4ss3ssm3nt", True, "assessment with leet"),
        # Underscore usage
        ("bad_4ss", False, "ass with underscore"),
        ("good_c1ass", True, "class with underscore"),
        ("my_p4ss", True, "pass with underscore"),
        # Case preservation tests
        ("MyUser123", True, "mixed case allowed"),
        ("SHOUT_99", True, "uppercase allowed"),
        ("quiet_1", True, "lowercase allowed"),
    ]

    print("Username Validation Tests:")
    print("=" * 70)
    print(f"{'Username':<20} {'Expected':<10} {'Result':<10} {'Reason'}")
    print("-" * 70)

    passed = 0
    total = len(test_cases)

    for username, expected_valid, description in test_cases:
        result = is_username_allowed(username)
        actual_valid = result.allowed

        # Check if result matches expectation
        test_passed = actual_valid == expected_valid
        status = "✓ PASS" if test_passed else "✗ FAIL"

        if test_passed:
            passed += 1

        print(f"{username:<20} {'ALLOW' if expected_valid else 'BLOCK':<10} {status:<10} {description}")

        if not result.allowed and result.reason:
            print(f"{'':20} {'':10} {'':10} -> {result.reason}")

        if result.canonical_form and result.canonical_form != username:
            print(f"{'':20} {'':10} {'':10} -> Canonical: {result.canonical_form}")

        if not test_passed:
            print(
                f"{'':20} {'':10} {'':10} -> EXPECTED: {'ALLOW' if expected_valid else 'BLOCK'}, GOT: {'ALLOW' if actual_valid else 'BLOCK'}"
            )

        print()

    print(f"Tests passed: {passed}/{total} ({passed/total*100:.1f}%)")

    # Demonstrate canonical form usage
    print("\nCanonical Form Examples (for uniqueness checking):")
    print("-" * 50)
    examples = ["MyUser123", "myuser123", "MYUSER123", "My_User_123"]
    for example in examples:
        canonical = get_canonical_username(example)
        print(f"{example:<15} -> {canonical}")
